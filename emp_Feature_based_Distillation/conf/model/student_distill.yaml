name: emp
phase: forecast

target_hint_distill:
  _target_: src.model.trainer_forecast_hint_distilation.hintBasedTrainer
  future_steps: 60
  historical_steps: 50
  pretrained: true
  pretrained_weights_teacher: ${hydra:runtime.cwd}/checkpoints/empm.ckpt
  pretrained_weights_student: ${pretrained_weights}
  lr: ${lr}
  weight_decay: ${weight_decay}
  epochs: ${epochs}
  warmup_epochs: ${warmup_epochs}

  student_model:
    _target_: src.model.emp.EMP
    embed_dim: 32
    encoder_depth: 2
    num_heads: 4
    mlp_ratio: 2.0
    qkv_bias: False
    drop_path: 0.2
    decoder: "mlp"
      # _target_: src.model.decoder.mlp_decoder.MLPDecoder
      # embed_dim: 64
      # num_modes: 3
      # hidden_dim: 128
  
  teacher_model:
    _target_: src.model.emp.EMP
    embed_dim: 128
    encoder_depth: 4
    num_heads: 8
    mlp_ratio: 4.0
    qkv_bias: False
    drop_path: 0.2
    decoder: "mlp"
      # _target_: src.model.decoder.mlp_decoder.MLPDecoder
      # embed_dim: 64
      # num_modes: 3
      # hidden_dim: 128

target_normal_training:
#student for benchmarking
  # _target_: src.model.trainer_forecast.Trainer
  # dim: 32
  # historical_steps: 50
  # future_steps: 60
  # encoder_depth: 2
  # num_heads: 4
  # mlp_ratio: 2.0
  # qkv_bias: False
  # drop_path: 0.2
  # pretrained_weights: ${pretrained_weights}
  # lr: ${lr}
  # weight_decay: ${weight_decay}
  # epochs: ${epochs}
  # warmup_epochs: ${warmup_epochs}
  # decoder: mlp
#teacher for benchmarking
  _target_: src.model.trainer_forecast.Trainer
  dim: 128
  historical_steps: 50
  future_steps: 60
  encoder_depth: 4
  num_heads: 8
  mlp_ratio: 4.0
  qkv_bias: False
  drop_path: 0.2
  pretrained_weights: ${pretrained_weights}
  lr: ${lr}
  weight_decay: ${weight_decay}
  epochs: ${epochs}
  warmup_epochs: ${warmup_epochs}
  decoder: mlp
