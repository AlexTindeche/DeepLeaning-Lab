{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda87e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoder type: mlp\n",
      "RUN EMP-M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReLU()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from hydra.utils import instantiate, to_absolute_path\n",
    "import torch\n",
    "from importlib import import_module\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "teacher_dict = {\n",
    "    \"_target_\": \"src.model.trainer_forecast.Trainer\",\n",
    "    \"historical_steps\": 50,\n",
    "    \"future_steps\": 60,\n",
    "    \"dim\": 128,\n",
    "    \"encoder_depth\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"mlp_ratio\": 4.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"drop_path\": 0.2,\n",
    "    \"attention_type\": \"standard\",  # \"standard\", \"linear\", \"performer\"\n",
    "    \"decoder\": \"mlp\",  # Decoder type: \"mlp\" or \"detr\"\n",
    "    \"decoder_embed_dim\": None,\n",
    "    \"decoder_num_modes\": None,\n",
    "    \"decoder_hidden_dim\": None,\n",
    "    \"pretrained_weights\": None,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 20,\n",
    "    \"warmup_epochs\": 10,\n",
    "}\n",
    "\n",
    "if teacher_dict[\"decoder\"] == \"mlp\":\n",
    "    checkpoint = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/checkpoints/empm.ckpt\"\n",
    "else:\n",
    "    checkpoint = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/checkpoints/empd.ckpt\"\n",
    "assert os.path.exists(checkpoint), f\"Checkpoint {checkpoint} does not exist\"\n",
    "\n",
    "model_path = teacher_dict[\"_target_\"]\n",
    "module = import_module(model_path[: model_path.rfind(\".\")])\n",
    "Model: pl.LightningModule = getattr(module, model_path[model_path.rfind(\".\") + 1 :])\n",
    "teacher = Model.load_from_checkpoint(checkpoint, **teacher_dict)\n",
    "\n",
    "# Cut the layer that gives the logits and keep the \"penultimate\" layer\n",
    "teacher.net.dense_predictor.pop(-1)  \n",
    "teacher.net.decoder.loc.pop(-1)\n",
    "teacher.net.decoder.loc.pop(-1)\n",
    "teacher.net.decoder.pi.pop(-1)\n",
    "teacher.net.decoder.pi.pop(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14602a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model instantiated with the following configuration:\n",
      "net: EMP(\n",
      "  (h_proj): Linear(in_features=5, out_features=128, bias=True)\n",
      "  (h_embed): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.067)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.067)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.133)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.133)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.200)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.200)\n",
      "    )\n",
      "  )\n",
      "  (lane_embed): LaneEmbeddingLayer(\n",
      "    (first_conv): Sequential(\n",
      "      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (second_conv): Sequential(\n",
      "      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (pos_embed): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.067)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.067)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.133)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.133)\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.200)\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.200)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (decoder): MultimodalDecoder(\n",
      "    (mode_embed): Embedding(6, 128)\n",
      "    (loc): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (pi): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dense_predictor): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "val_metrics: MetricCollection(\n",
      "  (MR): MR()\n",
      "  (brier-minFDE6): brierMinFDE()\n",
      "  (minADE1): minADE()\n",
      "  (minADE6): minADE()\n",
      "  (minFDE1): minFDE()\n",
      "  (minFDE6): minFDE(),\n",
      "  prefix=val_\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model layers\n",
    "print(\"Teacher model instantiated with the following configuration:\")\n",
    "for name, layer in teacher.named_children():\n",
    "    print(f\"{name}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f535032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoder type: mlp\n",
      "RUN EMP-M\n",
      "Student model instantiated with the following configuration:\n",
      "net: EMP(\n",
      "  (h_proj): Linear(in_features=5, out_features=64, bias=True)\n",
      "  (h_embed): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): PerformerAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): PerformerAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.100)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.100)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): PerformerAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.200)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.200)\n",
      "    )\n",
      "  )\n",
      "  (lane_embed): LaneEmbeddingLayer(\n",
      "    (first_conv): Sequential(\n",
      "      (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (second_conv): Sequential(\n",
      "      (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (pos_embed): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): GELU(approximate='none')\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): PerformerAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): PerformerAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.100)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.100)\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): PerformerAttention(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path1): DropPath(drop_prob=0.200)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path2): DropPath(drop_prob=0.200)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (decoder): MultimodalDecoder(\n",
      "    (mode_embed): Embedding(9, 64)\n",
      "    (loc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "    (pi): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dense_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n",
      "val_metrics: MetricCollection(\n",
      "  (MR): MR()\n",
      "  (brier-minFDE6): brierMinFDE()\n",
      "  (minADE1): minADE()\n",
      "  (minADE6): minADE()\n",
      "  (minFDE1): minFDE()\n",
      "  (minFDE6): minFDE(),\n",
      "  prefix=val_\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "student_dict = {\n",
    "    \"_target_\": \"src.model.trainer_forecast.Trainer\",\n",
    "    \"dim\": 64,\n",
    "    \"historical_steps\": 50,\n",
    "    \"future_steps\": 60,\n",
    "    \"encoder_depth\": 3,\n",
    "    \"num_heads\": 4,\n",
    "    \"mlp_ratio\": 2.0,\n",
    "    \"qkv_bias\": False,\n",
    "    \"drop_path\": 0.2,\n",
    "    \"attention_type\": \"performer\",  # \"standard\", \"linear\", \"performer\"\n",
    "    \"decoder\": \"mlp\",\n",
    "    \"decoder_embed_dim\": 64,\n",
    "    \"decoder_num_modes\": 9,\n",
    "    \"decoder_hidden_dim\": 256,\n",
    "    \"pretrained_weights\": None,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 60,\n",
    "    \"warmup_epochs\": 10,\n",
    "}\n",
    "\n",
    "student = instantiate(student_dict)\n",
    "# Remove the last layer from the encoder or decoder\n",
    "student.net.dense_predictor = student.net.dense_predictor[:-1]  \n",
    "student.net.decoder.loc.pop(-1)\n",
    "student.net.decoder.loc.pop(-1)\n",
    "student.net.decoder.pi.pop(-1)\n",
    "student.net.decoder.pi.pop(-1)\n",
    "\n",
    "\n",
    "# Print the model layers\n",
    "print(\"Student model instantiated with the following configuration:\")\n",
    "for name, layer in student.named_children():\n",
    "    print(f\"{name}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c057134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data root: /home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data/emp/train, total number of files: 199908\n",
      "data root: /home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data/emp/val, total number of files: 24988\n",
      "Using 50.0% of the training and validation datasets.\n",
      "Number of training samples: 99954 out of 199908\n",
      "Number of validation samples: 12494 out of 24988\n"
     ]
    }
   ],
   "source": [
    "# Create datamodule\n",
    "datamodule_dict = {\n",
    "    \"_target_\": \"src.datamodule.av2_datamodule.Av2DataModule\",\n",
    "    \"data_root\": \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data\",\n",
    "    \"data_folder\": \"emp\",\n",
    "    \"train_batch_size\": 64,\n",
    "    \"val_batch_size\": 64,\n",
    "    \"test_batch_size\": 64,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 18,\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "datamodule = instantiate(datamodule_dict)\n",
    "\n",
    "FRACTION = 0.5  \n",
    "\n",
    "datamodule.setup(stage=\"fit\", train_fraction=FRACTION, val_fraction=FRACTION, test_fraction=FRACTION)\n",
    "\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataloader.dataset)} out of {len(datamodule.full_train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataloader.dataset)} out of {len(datamodule.full_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1417b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82400251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tqdm import tqdm\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# teacher = teacher.to(device)\n",
    "# teacher.eval()\n",
    "# features_list_train = []\n",
    "# features_list_val = []\n",
    "# points_train = []\n",
    "# points_val = []\n",
    "# with torch.no_grad():\n",
    "#     for data in tqdm(train_dataloader):\n",
    "#         for k in data.keys():\n",
    "#             if torch.is_tensor(data[k]): data[k] = data[k].to(device)\n",
    "        \n",
    "#         points_train.append(data)\n",
    "        \n",
    "#         features = teacher.net.get_features(data)\n",
    "#         features_list_train.append(features)\n",
    "        \n",
    "        \n",
    "#     for data in tqdm(val_dataloader):\n",
    "#         for k in data.keys():\n",
    "#             if torch.is_tensor(data[k]): data[k] = data[k].to(device)\n",
    "        \n",
    "#         features = teacher.net.get_features(data)\n",
    "#         features_list_val.append(features)\n",
    "#         points_val.append(data)\n",
    "        \n",
    "# # Save the features to a file\n",
    "# import pickle as pkl\n",
    "# output_file_train = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/features_train.pkl\"\n",
    "# with open(output_file_train, \"wb\") as f:\n",
    "#     pkl.dump(features_list_train, f)\n",
    "    \n",
    "# output_file_val = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/features_val.pkl\"\n",
    "# with open(output_file_val, \"wb\") as f:\n",
    "#     pkl.dump(features_list_val, f)\n",
    "\n",
    "\n",
    "# output_file_data = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/data_points_train.pkl\"\n",
    "# with open(output_file_data, \"wb\") as f:\n",
    "#     pkl.dump(points_train, f)\n",
    "\n",
    "# output_file_data = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/data_points_val.pkl\"\n",
    "# with open(output_file_data, \"wb\") as f:\n",
    "#     pkl.dump(points_val, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abfa6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h-score: tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def h_score(t_feat, s_feat, tau=0.1, N_over_M=1.0):\n",
    "    \"\"\"\n",
    "    Compute h(T, S) as defined in Eq (19) in the paper.\n",
    "    \"\"\"\n",
    "    # Normalize features \n",
    "    t_feat = F.normalize(t_feat, dim=1)\n",
    "    s_feat = F.normalize(s_feat, dim=1)\n",
    "\n",
    "    # Similarity scores\n",
    "    sim = torch.sum(t_feat * s_feat, dim=1) / tau  # Exponent of e for numerator\n",
    "    numerator = torch.exp(sim)\n",
    "    denominator = numerator + N_over_M\n",
    "    h = numerator / denominator  # shape: (B,)\n",
    "    return h\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test_h_score():\n",
    "    # Create dummy features\n",
    "    t_feat = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    s_feat = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "    # Compute h-score\n",
    "    h = h_score(t_feat, s_feat)\n",
    "\n",
    "    # Print result\n",
    "    print(\"h-score:\", h)\n",
    "    # Expected output: h-score: tensor([1.0000, 1.0000, 1.0000])\n",
    "    assert torch.allclose(h, torch.tensor([1.0, 1.0, 1.0]), atol=1e-4), \"h-score test failed!\"\n",
    "    \n",
    "test_h_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfa2a810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic loss: tensor(    0.0001)\n"
     ]
    }
   ],
   "source": [
    "def critic_loss(t_pos, s_pos, t_neg, s_neg, tau=0.1, N=1.0, M = 1.0):\n",
    "    \"\"\"\n",
    "    Compute the full critic loss:\n",
    "    - t_pos, s_pos: matched teacher/student features (C=1)\n",
    "    - t_neg, s_neg: mismatched features (C=0)\n",
    "    \"\"\"\n",
    "    N_over_M = N / M if M != 0 else 1.0  # Avoid division by zero\n",
    "    \n",
    "    # Positive pair scores\n",
    "    h_pos = h_score(t_pos, s_pos, tau, N_over_M)\n",
    "    loss_pos = torch.median(torch.log(h_pos + 1e-8))  # Explicit Monte Carlo estimate\n",
    "\n",
    "    # Negative pair scores\n",
    "    h_neg = h_score(t_neg, s_neg, tau, N_over_M)\n",
    "    loss_neg = torch.median(torch.log(1 - h_neg + 1e-8))  # Explicit Monte Carlo estimate\n",
    "\n",
    "    # Combine as in Eq (18)\n",
    "    loss = -(loss_pos + N * loss_neg)\n",
    "    return loss\n",
    "\n",
    "# Test function\n",
    "def test_critic_loss():\n",
    "    # Here I tried to find two tensors that would yield a loss of almost 0\n",
    "    t_pos = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "    s_pos = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "\n",
    "    t_neg = torch.tensor([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "    s_neg = torch.tensor([[-1.0, 0.0], [0.0, -1.0], [-1.0, -1.0]])\n",
    "\n",
    "    # Compute critic loss\n",
    "    loss = critic_loss(t_pos, s_pos, t_neg, s_neg, tau=0.1)\n",
    "\n",
    "    # Print result\n",
    "    print(\"Critic loss:\", loss)\n",
    "    # Expected output: Critic loss: tensor(1.)\n",
    "    assert torch.isclose(loss, torch.tensor(0.), atol=1e-4), \"Critic loss test failed!\"\n",
    "    \n",
    "test_critic_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load features from the saved file\n",
    "# from tqdm import tqdm\n",
    "# import pickle as pkl\n",
    "\n",
    "\n",
    "# output_file_train = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/features_train.pkl\"\n",
    "# with open(output_file_train, \"rb\") as f:\n",
    "#     features_list_train = pkl.load(f)\n",
    "    \n",
    "# output_file_val = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/features_val.pkl\"\n",
    "# with open(output_file_val, \"rb\") as f:\n",
    "#     features_list_val = pkl.load(f)\n",
    "    \n",
    "# output_file_data = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/data_points_train.pkl\"\n",
    "# with open(output_file_data, \"rb\") as f:\n",
    "#     points_train = pkl.load(f)\n",
    "    \n",
    "# output_file_data = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/data_points_val.pkl\"\n",
    "# with open(output_file_data, \"rb\") as f:\n",
    "#     points_val = pkl.load(f)\n",
    "    \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dd87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# student.train()\n",
    "# student = student.to(device)\n",
    "# teacher.to(device)\n",
    "# teacher.eval()\n",
    "# batch_size = 64\n",
    "\n",
    "# # Already organized in batches\n",
    "\n",
    "\n",
    "# # If the embedding dim of the student is different from the teacher, add a projection layer\n",
    "# # Get a batch of data\n",
    "# batch = points_train[0]\n",
    "# for k in batch.keys():\n",
    "#     if torch.is_tensor(batch[k]): \n",
    "#         batch[k] = batch[k].to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     dim_teacher = teacher.net.get_features(batch).shape[1]\n",
    "#     dim_student = student.net.get_features(batch).shape[1]\n",
    "\n",
    "# print(f\"Teacher feature dimension: {dim_teacher}, Student feature dimension: {dim_student}\")\n",
    "# projection_layer = None\n",
    "# if dim_teacher != dim_student:\n",
    "#     projection_layer = torch.nn.Linear(dim_student, dim_teacher)\n",
    "#     projection_layer.to(device)\n",
    "#     print(f\"Projection layer added: {projection_layer}\")\n",
    "    \n",
    "#     # Freeze the projection layer\n",
    "#     for param in projection_layer.parameters():\n",
    "#         param.requires_grad = False\n",
    "    \n",
    "# optimizer, scheduler = student.configure_optimizers()\n",
    "# optimizer = optimizer[0] if isinstance(optimizer, list) else optimizer\n",
    "# scheduler = scheduler[0] if isinstance(scheduler, list) else scheduler\n",
    "    \n",
    "# # M = nr of positive pairs, N = nr of negative pairs\n",
    "# M = len(features_list_train[0])  # Number of positive pairs in a batch\n",
    "# N = len(features_list_train[0])  # Number of negative pairs in a batch\n",
    "# for i in range(10):\n",
    "#     losses = []\n",
    "#     print(f\"Epoch {i+1}/{10}\")\n",
    "#     for idx in tqdm(range(len(points_train))):\n",
    "#         batch = points_train[idx]\n",
    "#         for k in batch.keys():\n",
    "#             if torch.is_tensor(batch[k]): batch[k] = batch[k].to(device)\n",
    "#         # Eliminate \"y\" key if it exists\n",
    "#         if 'y' in batch:\n",
    "#             del batch['y']\n",
    "#         feature_contrast = student.net.get_features(batch)\n",
    "\n",
    "#         if projection_layer is not None:\n",
    "#             feature_contrast = projection_layer(feature_contrast)\n",
    "\n",
    "\n",
    "#         batch_feats = features_list_train[idx]\n",
    "\n",
    "#         # Get a random feature from the batch\n",
    "#         random_idx = random.randint(0, len(batch_feats) - 1)\n",
    "#         t_pos = batch_feats[random_idx].unsqueeze(0)  # [1, feature_dim]\n",
    "#         s_pos = feature_contrast[random_idx].unsqueeze(0)  # [1, feature_dim]\n",
    "        \n",
    "#         # Get a random negative feature from the batch\n",
    "#         neg_idx = random.randint(0, len(batch_feats) - 1)\n",
    "#         while neg_idx == random_idx:  # Ensure it's different from the positive index\n",
    "#             neg_idx = random.randint(0, len(batch_feats) - 1)\n",
    "#         t_neg = batch_feats[neg_idx].unsqueeze(0)  # [1, feature_dim]\n",
    "#         s_neg = feature_contrast[neg_idx].unsqueeze(0)  # [1, feature_dim]\n",
    "#         # Compute the critic loss\n",
    "#         loss = critic_loss(t_pos, s_pos, t_neg, s_neg, tau=0.1, N=N, M=M)\n",
    "#         # print(f\"Critic loss: {loss.item()}\")\n",
    "        \n",
    "#         # Backpropagate the loss\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         losses.append(loss.item())\n",
    "#     losses = np.mean(losses)\n",
    "#     print(f\"Epoch {i+1} losses: {losses}\")\n",
    "#     if scheduler is not None:\n",
    "#         if isinstance(scheduler, list):\n",
    "#             for sch in scheduler:\n",
    "#                 sch.step()\n",
    "#         else:\n",
    "#             scheduler.step()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a1a025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher feature dimension: 512, Student feature dimension: 384\n",
      "Learnable projection layer added: Linear(in_features=384, out_features=512, bias=True)\n"
     ]
    }
   ],
   "source": [
    "teacher.to(device)\n",
    "student.to(device)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k in batch.keys():\n",
    "    if torch.is_tensor(batch[k]): batch[k] = batch[k].to(device)\n",
    "# Find the expected output size\n",
    "with torch.no_grad():\n",
    "    dim_teacher = teacher.net.get_features(batch).shape[1]\n",
    "    dim_student = student.net.get_features(batch).shape[1]\n",
    "\n",
    "print(f\"Teacher feature dimension: {dim_teacher}, Student feature dimension: {dim_student}\")\n",
    "\n",
    "# Create learnable projection layer\n",
    "projection_layer = None\n",
    "if dim_teacher != dim_student:\n",
    "    projection_layer = torch.nn.Linear(dim_student, dim_teacher).to(device)\n",
    "    print(f\"Learnable projection layer added: {projection_layer}\")\n",
    "    \n",
    "    # # Freeze the projection layer\n",
    "    # for param in projection_layer.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c0bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting online contrastive training with custom loss and memory bank...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|▋         | 100/1562 [00:37<08:45,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Loss = 4.2903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|█▎        | 200/1562 [01:14<09:59,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200: Loss = 4.1856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  19%|█▉        | 300/1562 [01:53<08:06,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300: Loss = 3.8885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  26%|██▌       | 400/1562 [02:30<08:02,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Loss = 3.9247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  32%|███▏      | 500/1562 [03:09<07:11,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500: Loss = 3.5616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  38%|███▊      | 600/1562 [03:49<06:50,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600: Loss = 3.9076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▍     | 700/1562 [04:28<05:40,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700: Loss = 3.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  51%|█████     | 800/1562 [05:08<05:20,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Loss = 3.3985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 900/1562 [05:49<04:36,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900: Loss = 3.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▍   | 1000/1562 [06:30<03:56,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000: Loss = 2.9685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|███████   | 1100/1562 [07:12<03:18,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100: Loss = 2.9214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  77%|███████▋  | 1200/1562 [07:56<02:47,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Loss = 3.2257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 1300/1562 [08:40<02:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300: Loss = 3.5046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 1400/1562 [09:29<01:22,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400: Loss = 3.1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  96%|█████████▌| 1500/1562 [10:15<00:30,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500: Loss = 2.8062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1562/1562 [10:43<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 3.8118\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   6%|▋         | 100/1562 [00:47<12:08,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Loss = 3.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  13%|█▎        | 200/1562 [01:35<10:57,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200: Loss = 3.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  19%|█▉        | 300/1562 [02:24<10:36,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300: Loss = 3.6756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  26%|██▌       | 400/1562 [03:12<09:40,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Loss = 3.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  32%|███▏      | 500/1562 [04:02<09:08,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500: Loss = 2.4824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  38%|███▊      | 600/1562 [04:52<08:07,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600: Loss = 2.8384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  45%|████▍     | 700/1562 [05:43<07:38,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700: Loss = 3.9443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  51%|█████     | 800/1562 [06:35<06:39,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Loss = 2.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  58%|█████▊    | 900/1562 [07:27<06:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900: Loss = 3.5903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  64%|██████▍   | 1000/1562 [08:21<05:26,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000: Loss = 2.7193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  70%|███████   | 1100/1562 [09:14<04:17,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100: Loss = 2.7227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  77%|███████▋  | 1200/1562 [10:10<03:30,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Loss = 2.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  83%|████████▎ | 1300/1562 [11:05<02:30,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300: Loss = 3.1031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|████████▉ | 1400/1562 [12:02<01:35,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400: Loss = 2.8758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  96%|█████████▌| 1500/1562 [13:02<00:39,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500: Loss = 2.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1562/1562 [13:41<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average Loss: 3.0069\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   6%|▋         | 100/1562 [01:05<16:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Loss = 2.3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  13%|█▎        | 200/1562 [02:09<14:40,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200: Loss = 3.7220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  19%|█▉        | 300/1562 [03:12<13:18,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300: Loss = 2.8158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  26%|██▌       | 400/1562 [04:17<12:50,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Loss = 3.2722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  32%|███▏      | 500/1562 [05:23<11:48,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500: Loss = 3.1270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  38%|███▊      | 600/1562 [06:30<11:14,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600: Loss = 3.7962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  45%|████▍     | 700/1562 [07:38<09:55,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700: Loss = 2.4666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  51%|█████     | 800/1562 [08:47<08:57,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Loss = 2.3797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  58%|█████▊    | 900/1562 [09:57<08:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900: Loss = 2.3092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  64%|██████▍   | 1000/1562 [11:06<06:51,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000: Loss = 2.3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  70%|███████   | 1100/1562 [12:17<05:39,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100: Loss = 2.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  77%|███████▋  | 1200/1562 [13:28<04:35,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Loss = 3.2554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  83%|████████▎ | 1300/1562 [14:41<03:18,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300: Loss = 3.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  90%|████████▉ | 1400/1562 [15:55<01:53,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400: Loss = 2.1903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:  96%|█████████▌| 1500/1562 [17:07<00:46,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500: Loss = 2.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1562/1562 [17:52<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Average Loss: 2.7446\n",
      "Online contrastive training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "student.train()\n",
    "teacher.eval()\n",
    "\n",
    "memory_bank_size = 2048  # Reduced memory bank size for better management\n",
    "memory_buffer = []\n",
    "\n",
    "# Create optimizer\n",
    "if projection_layer is not None:\n",
    "    all_params = list(student.parameters()) + list(projection_layer.parameters())\n",
    "    optimizer = torch.optim.AdamW(all_params, lr=1e-4, weight_decay=1e-4)\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "def contrastive_loss_batch_online(student_feats, pos_features, neg_features=None, tau=0.1, NEG_RATIO=1.0):\n",
    "    \"\"\"\n",
    "    Online contrastive loss function using your custom critic loss\n",
    "    \"\"\"\n",
    "    batch_size = student_feats.shape[0]\n",
    "    \n",
    "    if batch_size == 1:\n",
    "        return F.mse_loss(student_feats, pos_features)\n",
    "    \n",
    "    # For negative pairs\n",
    "    if neg_features is not None and neg_features.shape[0] > 0:\n",
    "        # Sample negative features randomly based on ratio\n",
    "        num_negatives = min(int(batch_size * NEG_RATIO), neg_features.shape[0])\n",
    "        if num_negatives > 0:\n",
    "            neg_indices = torch.randperm(neg_features.shape[0])[:num_negatives]\n",
    "            \n",
    "            t_pos = pos_features  # [B, feature_dim]\n",
    "            s_pos = student_feats  # [B, feature_dim]\n",
    "            \n",
    "            t_neg = neg_features[neg_indices]  # [neg_samples, feature_dim]\n",
    "            \n",
    "            # Create corresponding student negatives by cycling through student features\n",
    "            s_neg_indices = torch.arange(num_negatives) % batch_size\n",
    "            s_neg = student_feats[s_neg_indices]  # [neg_samples, feature_dim]\n",
    "            \n",
    "            M = t_pos.shape[0]  # Number of positive pairs\n",
    "            N = t_neg.shape[0]  # Number of negative pairs\n",
    "            \n",
    "            loss = critic_loss(t_pos, s_pos, t_neg, s_neg, tau=tau, N=N, M=M)\n",
    "        else:\n",
    "            loss = F.mse_loss(student_feats, pos_features)\n",
    "    else:\n",
    "        # Fallback to MSE if no negatives available\n",
    "        loss = F.mse_loss(student_feats, pos_features)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "epochs = 3\n",
    "print(\"Starting online contrastive training with custom loss and memory bank...\")\n",
    "\n",
    "NEG_RATIO = 0.4\n",
    "\n",
    "losses = []\n",
    "LOSS_BUFFER_SIZE = 3000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # The training loop iterates directly over the train_dataloader\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        for k in batch.keys():\n",
    "            if torch.is_tensor(batch[k]): \n",
    "                batch[k] = batch[k].to(device)\n",
    "        \n",
    "        # Eliminate \"y\" key if it exists\n",
    "        if 'y' in batch:\n",
    "            del batch['y']\n",
    "        \n",
    "        # Get student features\n",
    "        student_features = student.net.get_features(batch)\n",
    "        \n",
    "        # Apply projection\n",
    "        if projection_layer is not None:\n",
    "            student_features = projection_layer(student_features)\n",
    "        \n",
    "        # Generate positive features on-the-fly from the teacher\n",
    "        with torch.no_grad():\n",
    "            pos_features = teacher.net.get_features(batch)\n",
    "        \n",
    "            # Update memory buffer with new features (FIFO queue)\n",
    "            pos_features_cpu = pos_features.cpu()\n",
    "            \n",
    "            # Concatenate new features to memory buffer\n",
    "            for feature in pos_features_cpu:\n",
    "                memory_buffer.append(feature)\n",
    "            \n",
    "            # Limit memory buffer size\n",
    "            if len(memory_buffer) > memory_bank_size:\n",
    "                memory_buffer.pop(0)\n",
    "        \n",
    "        # Prepare negative features for loss computation\n",
    "        neg_features_for_loss = None\n",
    "        if len(memory_buffer) > 0:\n",
    "            # Convert memory buffer back to GPU tensors for loss computation\n",
    "            neg_features_for_loss = torch.stack(memory_buffer).to(device)\n",
    "            \n",
    "        loss = contrastive_loss_batch_online(\n",
    "            student_features, \n",
    "            pos_features, \n",
    "            neg_features=neg_features_for_loss, \n",
    "            tau=0.05, \n",
    "            NEG_RATIO=NEG_RATIO\n",
    "        )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if len(losses) > LOSS_BUFFER_SIZE:\n",
    "            losses.pop(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if num_batches % 10 == 0:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            if len(losses) > 50:\n",
    "                window_size = min(50, len(losses) // 10)\n",
    "                smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "                smooth_x = np.arange(window_size-1, len(losses))\n",
    "                plt.plot(smooth_x, smoothed_losses, color='blue', linewidth=2, label=f'Moving Average (window={window_size})')\n",
    "            else:\n",
    "                plt.plot(np.arange(len(losses)), losses, color='lightblue', linewidth=1, label='Raw Loss')\n",
    "\n",
    "            \n",
    "            plt.xlabel('Batch')\n",
    "            plt.ylabel('Loss Value')\n",
    "            plt.title(f'Epoch {epoch + 1} Loss Progress (Moving Average)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"loss_distillation_smoothed.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"Batch {num_batches}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if num_batches > 0:\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch + 1} completed. No batches were processed.\")\n",
    "\n",
    "print(\"Online contrastive training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "751d75da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'student_trainableProj_loc_3ep_05tau.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save({\n",
    "    'student_state_dict': student.state_dict(),\n",
    "    'projection_layer_state_dict': projection_layer.state_dict() if projection_layer else None,\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}, 'student_trainableProj_loc_3ep_05tau.pth')\n",
    "\n",
    "print(\"Model saved as 'student_trainableProj_loc_3ep_05tau.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa89e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'student_trainableProj_locPiOthers.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# More training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstudent_trainableProj_locPiOthers.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m student\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstudent_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m projection_layer \u001b[38;5;129;01mand\u001b[39;00m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprojection_layer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/emp/lib/python3.9/site-packages/torch/serialization.py:1479\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1477\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1481\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1483\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/emp/lib/python3.9/site-packages/torch/serialization.py:759\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 759\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/emp/lib/python3.9/site-packages/torch/serialization.py:740\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'student_trainableProj_locPiOthers.pth'"
     ]
    }
   ],
   "source": [
    "# More training\n",
    "checkpoint = torch.load('student_trainableProj_locPiOthers.pth')\n",
    "student.load_state_dict(checkpoint['student_state_dict'])\n",
    "if projection_layer and checkpoint['projection_layer_state_dict']:\n",
    "    projection_layer.load_state_dict(checkpoint['projection_layer_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # The training loop iterates directly over the train_dataloader\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        for k in batch.keys():\n",
    "            if torch.is_tensor(batch[k]): \n",
    "                batch[k] = batch[k].to(device)\n",
    "        \n",
    "        # Eliminate \"y\" key if it exists\n",
    "        if 'y' in batch:\n",
    "            del batch['y']\n",
    "        \n",
    "        # Get student features\n",
    "        student_features = student.net.get_features(batch)\n",
    "        \n",
    "        # Apply projection\n",
    "        if projection_layer is not None:\n",
    "            student_features = projection_layer(student_features)\n",
    "        \n",
    "        # Generate positive features on-the-fly from the teacher\n",
    "        with torch.no_grad():\n",
    "            pos_features = teacher.net.get_features(batch)\n",
    "        \n",
    "            # Update memory buffer with new features (FIFO queue)\n",
    "            pos_features_cpu = pos_features.cpu()\n",
    "            \n",
    "            # Concatenate new features to memory buffer\n",
    "            for feature in pos_features_cpu:\n",
    "                memory_buffer.append(feature)\n",
    "            \n",
    "            # Limit memory buffer size\n",
    "            if len(memory_buffer) > memory_bank_size:\n",
    "                memory_buffer.pop(0)\n",
    "        \n",
    "        # Prepare negative features for loss computation\n",
    "        neg_features_for_loss = None\n",
    "        if len(memory_buffer) > 0:\n",
    "            # Convert memory buffer back to GPU tensors for loss computation\n",
    "            neg_features_for_loss = torch.stack(memory_buffer).to(device)\n",
    "            \n",
    "        loss = contrastive_loss_batch_online(\n",
    "            student_features, \n",
    "            pos_features, \n",
    "            neg_features=neg_features_for_loss, \n",
    "            tau=0.05, \n",
    "            NEG_RATIO=NEG_RATIO\n",
    "        )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if len(losses) > LOSS_BUFFER_SIZE:\n",
    "            losses.pop(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if num_batches % 10 == 0:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            if len(losses) > 50:\n",
    "                window_size = min(50, len(losses) // 10)\n",
    "                smoothed_losses = np.convolve(losses, np.ones(window_size)/window_size, mode='valid')\n",
    "                smooth_x = np.arange(window_size-1, len(losses))\n",
    "                plt.plot(smooth_x, smoothed_losses, color='blue', linewidth=2, label=f'Moving Average (window={window_size})')\n",
    "            else:\n",
    "                plt.plot(np.arange(len(losses)), losses, color='lightblue', linewidth=1, label='Raw Loss')\n",
    "\n",
    "            \n",
    "            plt.xlabel('Batch')\n",
    "            plt.ylabel('Loss Value')\n",
    "            plt.title(f'Epoch {epoch + 1} Loss Progress (Moving Average)')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"loss_distillation_smoothed.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"Batch {num_batches}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if num_batches > 0:\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch + 1} completed. No batches were processed.\")\n",
    "\n",
    "print(\"Online contrastive training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e79292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoder type: mlp\n",
      "RUN EMP-M\n",
      "Test loss after loading model: 3.1843\n"
     ]
    }
   ],
   "source": [
    "# Load the model for testing\n",
    "checkpoint = torch.load('student_trainableProj_loc_3ep_05tau.pth')\n",
    "student = instantiate(student_dict)\n",
    "# Remove the last layer from the encoder or decoder\n",
    "student.net.dense_predictor = student.net.dense_predictor[:-1]  \n",
    "student.net.decoder.loc = student.net.decoder.loc[:-1]\n",
    "student.net.decoder.pi = student.net.decoder.pi[:-1]\n",
    "student.load_state_dict(checkpoint['student_state_dict'])  # <-- FIXED LINE\n",
    "\n",
    "teacher.eval()\n",
    "student.eval()\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "\n",
    "batch_pos = next(iter(train_dataloader))\n",
    "for k in batch_pos.keys():\n",
    "    if torch.is_tensor(batch_pos[k]): \n",
    "        batch_pos[k] = batch_pos[k].to(device)\n",
    "# Eliminate \"y\" key if it exists\n",
    "if 'y' in batch_pos:\n",
    "    del batch_pos['y']\n",
    "    \n",
    "batch_neg = next(iter(train_dataloader))\n",
    "for k in batch_neg.keys():\n",
    "    if torch.is_tensor(batch_neg[k]): \n",
    "        batch_neg[k] = batch_neg[k].to(device)\n",
    "# Eliminate \"y\" key if it exists\n",
    "if 'y' in batch_neg:\n",
    "    del batch_neg['y']\n",
    "\n",
    "# After loading student and teacher and before loss calculation\n",
    "with torch.no_grad():\n",
    "    pos_teacher_features = teacher.net.get_features(batch_pos)\n",
    "    neg_teacher_features = teacher.net.get_features(batch_neg)\n",
    "    pos_student_features = student.net.get_features(batch_pos)\n",
    "\n",
    "    # Project student features if needed\n",
    "    if pos_teacher_features.shape[1] != pos_student_features.shape[1]:\n",
    "        projection_layer = torch.nn.Linear(pos_student_features.shape[1], pos_teacher_features.shape[1]).to(device)\n",
    "        # Optionally load projection weights if you saved them\n",
    "        if 'projection_layer_state_dict' in checkpoint and checkpoint['projection_layer_state_dict'] is not None:\n",
    "            projection_layer.load_state_dict(checkpoint['projection_layer_state_dict'])\n",
    "        pos_student_features = projection_layer(pos_student_features)\n",
    "\n",
    "loss = contrastive_loss_batch_online(\n",
    "    pos_student_features, \n",
    "    pos_teacher_features, \n",
    "    neg_features=neg_teacher_features, \n",
    "    tau=0.05, \n",
    "    NEG_RATIO=NEG_RATIO\n",
    ")\n",
    "print(f\"Test loss after loading model: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4c9644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoder type: mlp\n",
      "RUN EMP-M\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1562/1562 [02:25<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done.\n",
      "Average loss: 79.2203\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1562/1562 [02:26<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done.\n",
      "Average loss: 11.2786\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1562/1562 [02:23<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done.\n",
      "Average loss: 7.9010\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1562/1562 [02:24<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done.\n",
      "Average loss: 5.3608\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1562/1562 [02:25<00:00, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done.\n",
      "Average loss: 4.5518\n",
      "Fine-tuning complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "student = instantiate(student_dict)\n",
    "checkpoint = torch.load('student_trainableProj_locPiOthers.pth')\n",
    "student.load_state_dict(checkpoint['student_state_dict'], strict=False)\n",
    "\n",
    "student.to(device)\n",
    "student.train()\n",
    "\n",
    "optimizer, scheduler = student.configure_optimizers()\n",
    "optimizer = optimizer[0] if isinstance(optimizer, list) else optimizer\n",
    "scheduler = scheduler[0] if isinstance(scheduler, list) else scheduler\n",
    "\n",
    "dataloader = datamodule.train_dataloader()\n",
    "\n",
    "# Freeze all layers except the last one\n",
    "for name, param in student.named_parameters():\n",
    "    if 'dense_predictor' in name or 'decoder' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Epoch {epoch+1}/5\")\n",
    "    losses = []\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        for k in batch.keys():\n",
    "            if torch.is_tensor(batch[k]):\n",
    "                batch[k] = batch[k].to(device)\n",
    "        preds = student(batch)\n",
    "        # preds['y_hat']: (batch, 9, 60, 2), batch['y'][:, 0, :, :] : (batch, 60, 2)\n",
    "        gt_traj_ego = batch['y'][:, 0, :, :]  # (batch, 60, 2)\n",
    "        pred_modes = preds['y_hat']           # (batch, 9, 60, 2)\n",
    "        # Compute ADE for each mode\n",
    "        gt_expanded = gt_traj_ego.unsqueeze(1).expand(-1, pred_modes.shape[1], -1, -1)  # (batch, 9, 60, 2)\n",
    "        displacement_errors = torch.norm(pred_modes - gt_expanded, dim=-1)  # (batch, 9, 60)\n",
    "        ade_per_mode = displacement_errors.mean(dim=-1)  # (batch, 9)\n",
    "        best_mode_idx = ade_per_mode.argmin(dim=1)       # (batch,)\n",
    "        batch_indices = torch.arange(pred_modes.shape[0])\n",
    "        best_pred = pred_modes[batch_indices, best_mode_idx]  # (batch, 60, 2)\n",
    "        # Now compute loss\n",
    "        loss = torch.nn.functional.mse_loss(best_pred, gt_traj_ego)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    print(f\"Epoch {epoch+1} done.\")\n",
    "    print(f\"Average loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3eaa8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'student_trainableProj_loc_3ep_05tau_more.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save({\n",
    "    'student_state_dict': student.state_dict(),\n",
    "    'projection_layer_state_dict': projection_layer.state_dict() if projection_layer else None,\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "}, 'student_trainableProj_loc_3ep_05tau_more.pth')\n",
    "\n",
    "print(\"Model saved as 'student_trainableProj_loc_3ep_05tau_more.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a48918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoder type: mlp\n",
      "RUN EMP-M\n",
      "Loaded trained weights (non-strict mode)\n",
      "data root: /home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data/emp/train, total number of files: 199908\n",
      "data root: /home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data/emp/val, total number of files: 24988\n",
      "Using 10.0% of the training and validation datasets.\n",
      "Evaluating trajectory prediction model...\n",
      "Inspection of predictions and ground truth:\n",
      "Prediction type: <class 'dict'>\n",
      "  y_hat: torch.Size([64, 9, 60, 2])\n",
      "  pi: torch.Size([64, 9])\n",
      "  y_hat_others: torch.Size([64, 48, 60, 2])\n",
      "  y_hat_eps: torch.Size([64, 9, 2])\n",
      "  x_agent: torch.Size([64, 64])\n",
      "Ground truth 'y' shape: torch.Size([64, 49, 60, 2])\n",
      "Validation MSE (best mode): 3.9397 ± 1.2164\n",
      "Validation MAE (best mode): 1.1643 ± 0.1050\n",
      "Validation ADE (best mode): 1.8989 ± 1.2829\n",
      "Validation FDE (best mode): 4.3558 ± 3.5862\n",
      "\n",
      "Evaluating using prediction confidence weights...\n",
      "Confidence-based MSE: 118.1811 ± 14.9803\n",
      "Confidence-based MAE: 6.3760 ± 0.3610\n",
      "Confidence-based ADE: 11.2365 ± 6.3482\n",
      "Confidence-based FDE: 24.3001 ± 13.7967\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint without the logit layers\n",
    "checkpoint_path = 'student_trainableProj_loc_3ep_05tau_more.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Create a fresh student model with full layers\n",
    "student_full = instantiate(student_dict)\n",
    "\n",
    "# Load the trained weights\n",
    "try:\n",
    "    student_full.load_state_dict(checkpoint['student_state_dict'], strict=False)\n",
    "    print(\"Loaded trained weights (non-strict mode)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {e}\")\n",
    "    model_dict = student_full.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in checkpoint['student_state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    student_full.load_state_dict(model_dict)\n",
    "    print(\"Loaded compatible weights manually\")\n",
    "\n",
    "student_full = student_full.to(device)\n",
    "student_full.eval()\n",
    "\n",
    "# torch.save({\n",
    "#     'model_state_dict': student_full.state_dict(),\n",
    "#     'model_config': student_dict,\n",
    "#     'training_info': {\n",
    "#         'epochs_trained': checkpoint.get('epoch', 'unknown'),\n",
    "#         'architecture': 'student_with_contrastive_learning'\n",
    "#     }\n",
    "# }, 'student_model_full_with_predictions.pth')\n",
    "\n",
    "datamodule.setup(stage=\"validate\", train_fraction=0.1, val_fraction=0.1, test_fraction=0.1)\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "\n",
    "print(\"Evaluating trajectory prediction model...\")\n",
    "\n",
    "# First, let's inspect the prediction structure\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "for k in sample_batch.keys():\n",
    "    if torch.is_tensor(sample_batch[k]):\n",
    "        sample_batch[k] = sample_batch[k].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_preds = student_full(sample_batch)\n",
    "\n",
    "print(\"Inspection of predictions and ground truth:\")\n",
    "print(f\"Prediction type: {type(sample_preds)}\")\n",
    "if isinstance(sample_preds, dict):\n",
    "    for key, value in sample_preds.items():\n",
    "        if torch.is_tensor(value):\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "print(f\"Ground truth 'y' shape: {sample_batch['y'].shape}\")\n",
    "\n",
    "# Multi-modal trajectory prediction evaluation\n",
    "mse_errors = []\n",
    "mae_errors = []\n",
    "ade_errors = []\n",
    "fde_errors = []\n",
    "\n",
    "def evaluate_multimodal_prediction(pred_modes, gt_traj_ego):\n",
    "    \"\"\"\n",
    "    Evaluate multi-modal prediction by selecting the best mode\n",
    "    pred_modes: (batch, num_modes, time, 2)\n",
    "    gt_traj_ego: (batch, time, 2)\n",
    "    \"\"\"\n",
    "    batch_size, num_modes, time_steps, _ = pred_modes.shape\n",
    "    \n",
    "    # Compute ADE for each mode\n",
    "    # Expand gt_traj_ego to match pred_modes shape\n",
    "    gt_expanded = gt_traj_ego.unsqueeze(1).expand(-1, num_modes, -1, -1)  # (batch, num_modes, time, 2)\n",
    "    \n",
    "    # Compute displacement errors for all modes\n",
    "    displacement_errors = torch.norm(pred_modes - gt_expanded, dim=-1)  # (batch, num_modes, time)\n",
    "    \n",
    "    # ADE for each mode: average over time\n",
    "    ade_per_mode = displacement_errors.mean(dim=-1)  # (batch, num_modes)\n",
    "    \n",
    "    # FDE for each mode: final time step\n",
    "    fde_per_mode = displacement_errors[:, :, -1]  # (batch, num_modes)\n",
    "    \n",
    "    # Select best mode (lowest ADE)\n",
    "    best_mode_idx = ade_per_mode.argmin(dim=1)  # (batch,)\n",
    "    \n",
    "    # Extract best ADE and FDE\n",
    "    batch_indices = torch.arange(batch_size)\n",
    "    best_ade = ade_per_mode[batch_indices, best_mode_idx]  # (batch,)\n",
    "    best_fde = fde_per_mode[batch_indices, best_mode_idx]  # (batch,)\n",
    "    \n",
    "    # Get best trajectory for MSE/MAE computation\n",
    "    best_traj = pred_modes[batch_indices, best_mode_idx]  # (batch, time, 2)\n",
    "    \n",
    "    return best_traj, best_ade, best_fde\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    for k in batch.keys():\n",
    "        if torch.is_tensor(batch[k]):\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = student_full(batch)\n",
    "        \n",
    "        # Extract multi-modal predictions\n",
    "        if isinstance(preds, dict) and 'y_hat' in preds:\n",
    "            pred_modes = preds['y_hat']  # (batch, 9, 60, 2)\n",
    "        else:\n",
    "            print(\"Could not find 'y_hat' in predictions\")\n",
    "            break\n",
    "        \n",
    "        # Extract ego agent ground truth (first agent, index 0)\n",
    "        gt_traj_all = batch['y']  # (batch, 42, 60, 2)\n",
    "        gt_traj_ego = gt_traj_all[:, 0, :, :]  # (batch, 60, 2) - ego agent only\n",
    "        \n",
    "        # Evaluate multi-modal prediction\n",
    "        best_traj, batch_ade, batch_fde = evaluate_multimodal_prediction(pred_modes, gt_traj_ego)\n",
    "        \n",
    "        # Compute MSE and MAE using best trajectory\n",
    "        mse = torch.nn.functional.mse_loss(best_traj, gt_traj_ego)\n",
    "        mae = torch.nn.functional.l1_loss(best_traj, gt_traj_ego)\n",
    "        \n",
    "        mse_errors.append(mse.item())\n",
    "        mae_errors.append(mae.item())\n",
    "        ade_errors.extend(batch_ade.cpu().numpy())\n",
    "        fde_errors.extend(batch_fde.cpu().numpy())\n",
    "\n",
    "print(f\"Validation MSE (best mode): {np.mean(mse_errors):.4f} ± {np.std(mse_errors):.4f}\")\n",
    "print(f\"Validation MAE (best mode): {np.mean(mae_errors):.4f} ± {np.std(mae_errors):.4f}\")\n",
    "print(f\"Validation ADE (best mode): {np.mean(ade_errors):.4f} ± {np.std(ade_errors):.4f}\")\n",
    "print(f\"Validation FDE (best mode): {np.mean(fde_errors):.4f} ± {np.std(fde_errors):.4f}\")\n",
    "\n",
    "# Additional analysis: evaluate using prediction confidence (pi weights)\n",
    "if 'pi' in sample_preds:\n",
    "    print(\"\\nEvaluating using prediction confidence weights...\")\n",
    "    conf_mse_errors = []\n",
    "    conf_mae_errors = []\n",
    "    conf_ade_errors = []\n",
    "    conf_fde_errors = []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        for k in batch.keys():\n",
    "            if torch.is_tensor(batch[k]):\n",
    "                batch[k] = batch[k].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = student_full(batch)\n",
    "            pred_modes = preds['y_hat']  # (batch, 9, 60, 2)\n",
    "            pi_weights = preds['pi']  # (batch, 9) - confidence scores\n",
    "            \n",
    "            gt_traj_all = batch['y']\n",
    "            gt_traj_ego = gt_traj_all[:, 0, :, :]  # (batch, 60, 2)\n",
    "            \n",
    "            # Select mode with highest confidence\n",
    "            best_conf_idx = pi_weights.argmax(dim=1)  # (batch,)\n",
    "            batch_indices = torch.arange(pred_modes.shape[0])\n",
    "            conf_best_traj = pred_modes[batch_indices, best_conf_idx]  # (batch, 60, 2)\n",
    "            \n",
    "            # Compute metrics\n",
    "            conf_mse = torch.nn.functional.mse_loss(conf_best_traj, gt_traj_ego)\n",
    "            conf_mae = torch.nn.functional.l1_loss(conf_best_traj, gt_traj_ego)\n",
    "            \n",
    "            # ADE/FDE for confidence-based selection\n",
    "            displacement_errors = torch.norm(conf_best_traj - gt_traj_ego, dim=-1)\n",
    "            conf_ade = displacement_errors.mean(dim=-1)\n",
    "            conf_fde = displacement_errors[:, -1]\n",
    "            \n",
    "            conf_mse_errors.append(conf_mse.item())\n",
    "            conf_mae_errors.append(conf_mae.item())\n",
    "            conf_ade_errors.extend(conf_ade.cpu().numpy())\n",
    "            conf_fde_errors.extend(conf_fde.cpu().numpy())\n",
    "    \n",
    "    print(f\"Confidence-based MSE: {np.mean(conf_mse_errors):.4f} ± {np.std(conf_mse_errors):.4f}\")\n",
    "    print(f\"Confidence-based MAE: {np.mean(conf_mae_errors):.4f} ± {np.std(conf_mae_errors):.4f}\")\n",
    "    print(f\"Confidence-based ADE: {np.mean(conf_ade_errors):.4f} ± {np.std(conf_ade_errors):.4f}\")\n",
    "    print(f\"Confidence-based FDE: {np.mean(conf_fde_errors):.4f} ± {np.std(conf_fde_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2959d421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using decoder type: mlp\n",
      "RUN EMP-M\n",
      "data root: /home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data/emp/train, total number of files: 199908\n",
      "data root: /home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/data/emp/val, total number of files: 24988\n",
      "Using 10.0% of the training and validation datasets.\n",
      "Evaluating trajectory prediction model...\n",
      "Inspection of predictions and ground truth:\n",
      "Prediction type: <class 'dict'>\n",
      "  y_hat: torch.Size([64, 6, 60, 2])\n",
      "  pi: torch.Size([64, 6])\n",
      "  y_hat_others: torch.Size([64, 44, 60, 2])\n",
      "  y_hat_eps: torch.Size([64, 6, 2])\n",
      "  x_agent: torch.Size([64, 128])\n",
      "Ground truth 'y' shape: torch.Size([64, 45, 60, 2])\n",
      "Validation MSE (best mode): 0.8327 ± 0.6760\n",
      "Validation MAE (best mode): 0.4323 ± 0.0488\n",
      "Validation ADE (best mode): 0.7080 ± 0.7275\n",
      "Validation FDE (best mode): 1.6843 ± 2.0672\n",
      "\n",
      "Evaluating using prediction confidence weights...\n",
      "Confidence-based MSE: 5.3941 ± 2.0615\n",
      "Confidence-based MAE: 0.9974 ± 0.1312\n",
      "Confidence-based ADE: 1.7182 ± 1.8246\n",
      "Confidence-based FDE: 4.3709 ± 5.1262\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint without the logit layers\n",
    "checkpoint_path = 'student_model_custom_contrastive.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# # Create a fresh student model with full layers\n",
    "# student_full = instantiate(student_dict)\n",
    "\n",
    "# # Load the trained weights\n",
    "# try:\n",
    "#     student_full.load_state_dict(checkpoint['student_state_dict'], strict=False)\n",
    "#     print(\"Loaded trained weights (non-strict mode)\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading weights: {e}\")\n",
    "#     model_dict = student_full.state_dict()\n",
    "#     pretrained_dict = {k: v for k, v in checkpoint['student_state_dict'].items() if k in model_dict}\n",
    "#     model_dict.update(pretrained_dict)\n",
    "#     student_full.load_state_dict(model_dict)\n",
    "#     print(\"Loaded compatible weights manually\")\n",
    "\n",
    "# student_full = student_full.to(device)\n",
    "# student_full.eval()\n",
    "\n",
    "# torch.save({\n",
    "#     'model_state_dict': student_full.state_dict(),\n",
    "#     'model_config': student_dict,\n",
    "#     'training_info': {\n",
    "#         'epochs_trained': checkpoint.get('epoch', 'unknown'),\n",
    "#         'architecture': 'student_with_contrastive_learning'\n",
    "#     }\n",
    "# }, 'student_model_full_with_predictions.pth')\n",
    "\n",
    "if teacher_dict[\"decoder\"] == \"mlp\":\n",
    "    checkpoint = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/checkpoints/empm.ckpt\"\n",
    "else:\n",
    "    checkpoint = \"/home/alex/UNI EMERGENCY/Project/DeepLeaning-Lab/emp-main/checkpoints/empd.ckpt\"\n",
    "assert os.path.exists(checkpoint), f\"Checkpoint {checkpoint} does not exist\"\n",
    "\n",
    "model_path = teacher_dict[\"_target_\"]\n",
    "module = import_module(model_path[: model_path.rfind(\".\")])\n",
    "Model: pl.LightningModule = getattr(module, model_path[model_path.rfind(\".\") + 1 :])\n",
    "teacher = Model.load_from_checkpoint(checkpoint, **teacher_dict)\n",
    "teacher.to(device)\n",
    "teacher.eval()\n",
    "\n",
    "datamodule.setup(stage=\"validate\", train_fraction=0.1, val_fraction=0.1, test_fraction=0.1)\n",
    "val_dataloader = datamodule.val_dataloader()\n",
    "\n",
    "print(\"Evaluating trajectory prediction model...\")\n",
    "\n",
    "# First, let's inspect the prediction structure\n",
    "sample_batch = next(iter(val_dataloader))\n",
    "for k in sample_batch.keys():\n",
    "    if torch.is_tensor(sample_batch[k]):\n",
    "        sample_batch[k] = sample_batch[k].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_preds = teacher(sample_batch)\n",
    "\n",
    "print(\"Inspection of predictions and ground truth:\")\n",
    "print(f\"Prediction type: {type(sample_preds)}\")\n",
    "if isinstance(sample_preds, dict):\n",
    "    for key, value in sample_preds.items():\n",
    "        if torch.is_tensor(value):\n",
    "            print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "print(f\"Ground truth 'y' shape: {sample_batch['y'].shape}\")\n",
    "\n",
    "# Multi-modal trajectory prediction evaluation\n",
    "mse_errors = []\n",
    "mae_errors = []\n",
    "ade_errors = []\n",
    "fde_errors = []\n",
    "\n",
    "def evaluate_multimodal_prediction(pred_modes, gt_traj_ego):\n",
    "    \"\"\"\n",
    "    Evaluate multi-modal prediction by selecting the best mode\n",
    "    pred_modes: (batch, num_modes, time, 2)\n",
    "    gt_traj_ego: (batch, time, 2)\n",
    "    \"\"\"\n",
    "    batch_size, num_modes, time_steps, _ = pred_modes.shape\n",
    "    \n",
    "    # Compute ADE for each mode\n",
    "    # Expand gt_traj_ego to match pred_modes shape\n",
    "    gt_expanded = gt_traj_ego.unsqueeze(1).expand(-1, num_modes, -1, -1)  # (batch, num_modes, time, 2)\n",
    "    \n",
    "    # Compute displacement errors for all modes\n",
    "    displacement_errors = torch.norm(pred_modes - gt_expanded, dim=-1)  # (batch, num_modes, time)\n",
    "    \n",
    "    # ADE for each mode: average over time\n",
    "    ade_per_mode = displacement_errors.mean(dim=-1)  # (batch, num_modes)\n",
    "    \n",
    "    # FDE for each mode: final time step\n",
    "    fde_per_mode = displacement_errors[:, :, -1]  # (batch, num_modes)\n",
    "    \n",
    "    # Select best mode (lowest ADE)\n",
    "    best_mode_idx = ade_per_mode.argmin(dim=1)  # (batch,)\n",
    "    \n",
    "    # Extract best ADE and FDE\n",
    "    batch_indices = torch.arange(batch_size)\n",
    "    best_ade = ade_per_mode[batch_indices, best_mode_idx]  # (batch,)\n",
    "    best_fde = fde_per_mode[batch_indices, best_mode_idx]  # (batch,)\n",
    "    \n",
    "    # Get best trajectory for MSE/MAE computation\n",
    "    best_traj = pred_modes[batch_indices, best_mode_idx]  # (batch, time, 2)\n",
    "    \n",
    "    return best_traj, best_ade, best_fde\n",
    "\n",
    "for batch in val_dataloader:\n",
    "    for k in batch.keys():\n",
    "        if torch.is_tensor(batch[k]):\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = teacher(batch)\n",
    "        \n",
    "        # Extract multi-modal predictions\n",
    "        if isinstance(preds, dict) and 'y_hat' in preds:\n",
    "            pred_modes = preds['y_hat']  # (batch, 9, 60, 2)\n",
    "        else:\n",
    "            print(\"Could not find 'y_hat' in predictions\")\n",
    "            break\n",
    "        \n",
    "        # Extract ego agent ground truth (first agent, index 0)\n",
    "        gt_traj_all = batch['y']  # (batch, 42, 60, 2)\n",
    "        gt_traj_ego = gt_traj_all[:, 0, :, :]  # (batch, 60, 2) - ego agent only\n",
    "        \n",
    "        # Evaluate multi-modal prediction\n",
    "        best_traj, batch_ade, batch_fde = evaluate_multimodal_prediction(pred_modes, gt_traj_ego)\n",
    "        \n",
    "        # Compute MSE and MAE using best trajectory\n",
    "        mse = torch.nn.functional.mse_loss(best_traj, gt_traj_ego)\n",
    "        mae = torch.nn.functional.l1_loss(best_traj, gt_traj_ego)\n",
    "        \n",
    "        mse_errors.append(mse.item())\n",
    "        mae_errors.append(mae.item())\n",
    "        ade_errors.extend(batch_ade.cpu().numpy())\n",
    "        fde_errors.extend(batch_fde.cpu().numpy())\n",
    "\n",
    "print(f\"Validation MSE (best mode): {np.mean(mse_errors):.4f} ± {np.std(mse_errors):.4f}\")\n",
    "print(f\"Validation MAE (best mode): {np.mean(mae_errors):.4f} ± {np.std(mae_errors):.4f}\")\n",
    "print(f\"Validation ADE (best mode): {np.mean(ade_errors):.4f} ± {np.std(ade_errors):.4f}\")\n",
    "print(f\"Validation FDE (best mode): {np.mean(fde_errors):.4f} ± {np.std(fde_errors):.4f}\")\n",
    "\n",
    "# Additional analysis: evaluate using prediction confidence (pi weights)\n",
    "if 'pi' in sample_preds:\n",
    "    print(\"\\nEvaluating using prediction confidence weights...\")\n",
    "    conf_mse_errors = []\n",
    "    conf_mae_errors = []\n",
    "    conf_ade_errors = []\n",
    "    conf_fde_errors = []\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        for k in batch.keys():\n",
    "            if torch.is_tensor(batch[k]):\n",
    "                batch[k] = batch[k].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = teacher(batch)\n",
    "            pred_modes = preds['y_hat']  # (batch, 9, 60, 2)\n",
    "            pi_weights = preds['pi']  # (batch, 9) - confidence scores\n",
    "            \n",
    "            gt_traj_all = batch['y']\n",
    "            gt_traj_ego = gt_traj_all[:, 0, :, :]  # (batch, 60, 2)\n",
    "            \n",
    "            # Select mode with highest confidence\n",
    "            best_conf_idx = pi_weights.argmax(dim=1)  # (batch,)\n",
    "            batch_indices = torch.arange(pred_modes.shape[0])\n",
    "            conf_best_traj = pred_modes[batch_indices, best_conf_idx]  # (batch, 60, 2)\n",
    "            \n",
    "            # Compute metrics\n",
    "            conf_mse = torch.nn.functional.mse_loss(conf_best_traj, gt_traj_ego)\n",
    "            conf_mae = torch.nn.functional.l1_loss(conf_best_traj, gt_traj_ego)\n",
    "            \n",
    "            # ADE/FDE for confidence-based selection\n",
    "            displacement_errors = torch.norm(conf_best_traj - gt_traj_ego, dim=-1)\n",
    "            conf_ade = displacement_errors.mean(dim=-1)\n",
    "            conf_fde = displacement_errors[:, -1]\n",
    "            \n",
    "            conf_mse_errors.append(conf_mse.item())\n",
    "            conf_mae_errors.append(conf_mae.item())\n",
    "            conf_ade_errors.extend(conf_ade.cpu().numpy())\n",
    "            conf_fde_errors.extend(conf_fde.cpu().numpy())\n",
    "    \n",
    "    print(f\"Confidence-based MSE: {np.mean(conf_mse_errors):.4f} ± {np.std(conf_mse_errors):.4f}\")\n",
    "    print(f\"Confidence-based MAE: {np.mean(conf_mae_errors):.4f} ± {np.std(conf_mae_errors):.4f}\")\n",
    "    print(f\"Confidence-based ADE: {np.mean(conf_ade_errors):.4f} ± {np.std(conf_ade_errors):.4f}\")\n",
    "    print(f\"Confidence-based FDE: {np.mean(conf_fde_errors):.4f} ± {np.std(conf_fde_errors):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
